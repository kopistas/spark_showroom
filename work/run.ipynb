{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a19aa6a6-933f-4e01-80ea-f3984a2e922c",
   "metadata": {},
   "source": [
    "Note: На данном этапе в sources БД загружены данные. Создана структура таблиц для DWH и витрины"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592713a8-df9b-4605-b359-2310e1a004cc",
   "metadata": {},
   "source": [
    "Note 2: У меня не завелось со структурами в DWH из задания, я их просто с нуля сделал прям свои и загрузил через DBeaver.\n",
    "\n",
    "\n",
    "```\n",
    "-- Создание схемы DWH если её еще нет\n",
    "CREATE SCHEMA IF NOT EXISTS dwh;\n",
    "\n",
    "-- Таблица измерений customers\n",
    "CREATE TABLE dwh.d_customers (\n",
    "    customer_id BIGINT PRIMARY KEY,\n",
    "    customer_name VARCHAR,\n",
    "    customer_address VARCHAR,\n",
    "    customer_birthday DATE,\n",
    "    customer_email VARCHAR NOT NULL,\n",
    "    valid_from TIMESTAMP NOT NULL,\n",
    "    valid_to TIMESTAMP,\n",
    "    is_current BOOLEAN NOT NULL,\n",
    "    CONSTRAINT d_customers_email_uk UNIQUE (customer_email)\n",
    ");\n",
    "\n",
    "COMMENT ON TABLE dwh.d_customers IS 'Таблица измерений с информацией о заказчиках';\n",
    "\n",
    "-- Таблица измерений products\n",
    "CREATE TABLE dwh.d_products (\n",
    "    product_id BIGINT PRIMARY KEY,\n",
    "    product_name VARCHAR NOT NULL,\n",
    "    product_description VARCHAR NOT NULL,\n",
    "    product_type VARCHAR NOT NULL,\n",
    "    product_price BIGINT NOT NULL,\n",
    "    valid_from TIMESTAMP NOT NULL,\n",
    "    valid_to TIMESTAMP,\n",
    "    is_current BOOLEAN NOT NULL\n",
    ");\n",
    "\n",
    "COMMENT ON TABLE dwh.d_products IS 'Таблица измерений с информацией о продуктах';\n",
    "\n",
    "-- Таблица измерений craftsmans\n",
    "CREATE TABLE dwh.d_craftsmans (\n",
    "    craftsman_id BIGINT PRIMARY KEY,\n",
    "    craftsman_name VARCHAR NOT NULL,\n",
    "    craftsman_address VARCHAR NOT NULL,\n",
    "    craftsman_birthday DATE NOT NULL,\n",
    "    craftsman_email VARCHAR NOT NULL,\n",
    "    valid_from TIMESTAMP NOT NULL,\n",
    "    valid_to TIMESTAMP,\n",
    "    is_current BOOLEAN NOT NULL,\n",
    "    CONSTRAINT d_craftsmans_email_uk UNIQUE (craftsman_email)\n",
    ");\n",
    "\n",
    "COMMENT ON TABLE dwh.d_craftsmans IS 'Таблица измерений с информацией о мастерах';\n",
    "\n",
    "-- Таблица фактов orders\n",
    "CREATE TABLE dwh.f_orders (\n",
    "    order_id BIGINT PRIMARY KEY,\n",
    "    customer_id BIGINT NOT NULL,\n",
    "    craftsman_id BIGINT NOT NULL,\n",
    "    product_id BIGINT NOT NULL,\n",
    "    order_created_date DATE,\n",
    "    order_completion_date DATE,\n",
    "    order_status VARCHAR NOT NULL,\n",
    "    load_dttm TIMESTAMP NOT NULL,\n",
    "    CONSTRAINT f_orders_customer_fk FOREIGN KEY (customer_id) REFERENCES dwh.d_customers(customer_id),\n",
    "    CONSTRAINT f_orders_craftsman_fk FOREIGN KEY (craftsman_id) REFERENCES dwh.d_craftsmans(craftsman_id),\n",
    "    CONSTRAINT f_orders_product_fk FOREIGN KEY (product_id) REFERENCES dwh.d_products(product_id)\n",
    ");\n",
    "\n",
    "COMMENT ON TABLE dwh.f_orders IS 'Таблица фактов с информацией о заказах';\n",
    "\n",
    "-- Создание индексов для оптимизации производительности\n",
    "CREATE INDEX idx_f_orders_customer ON dwh.f_orders(customer_id);\n",
    "CREATE INDEX idx_f_orders_craftsman ON dwh.f_orders(craftsman_id);\n",
    "CREATE INDEX idx_f_orders_product ON dwh.f_orders(product_id);\n",
    "CREATE INDEX idx_f_orders_created_date ON dwh.f_orders(order_created_date);\n",
    "CREATE INDEX idx_f_orders_completion_date ON dwh.f_orders(order_completion_date);\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5148b794-ebf9-4206-ad21-2eea4ae51044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задание 8. Выгрузка данных в DWH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9a1096e5-eb57-470e-a114-b078309e0a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read source1_wide: 999\n",
      "Read source2_masters: 999\n",
      "Read source2_orders: 999\n",
      "Read source3_craftsmans: 999\n",
      "Read source3_customers: 999\n",
      "Read source3_orders: 999\n",
      "Processing dimension tables...\n",
      "Products before deduplication: 2997\n",
      "Products after deduplication: 2994\n",
      "Processing fact table...\n",
      "Total orders before deduplication: 2997\n",
      "Number of duplicated order keys: 0\n",
      "Total orders after deduplication: 2997\n",
      "Final orders count in f_orders: 2997\n",
      "Upserting dimension tables to DWH...\n",
      "Dimension table dwh.d_customers not found. Creating a new one.\n",
      "Inserted 2997 rows into dwh.d_customers\n",
      "Dimension table dwh.d_craftsmans not found. Creating a new one.\n",
      "Inserted 2997 rows into dwh.d_craftsmans\n",
      "Dimension table dwh.d_products not found. Creating a new one.\n",
      "Inserted 2994 rows into dwh.d_products\n",
      "Upserting fact table to DWH...\n",
      "Fact table dwh.f_orders not found. Creating a new one.\n",
      "Inserted 2997 rows into dwh.f_orders\n",
      "DWH load completed successfully (incremental)!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import TimestampType\n",
    "import datetime\n",
    "\n",
    "def log_count(df, message):\n",
    "    count = df.count()\n",
    "    print(f\"{message}: {count}\")\n",
    "    return count\n",
    "\n",
    "def init_spark():\n",
    "    \"\"\"Initialize Spark session\"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"DWH Data Load\") \\\n",
    "        .config(\"spark.jars\", \"postgresql-42.7.4.jar\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "def read_source_tables(spark, jdbc_url, db_properties):\n",
    "    source_tables = {\n",
    "        'source1_wide': spark.read.jdbc(jdbc_url, \"source1.craft_market_wide\", properties=db_properties),\n",
    "        'source2_masters': spark.read.jdbc(jdbc_url, \"source2.craft_market_masters_products\", properties=db_properties),\n",
    "        'source2_orders': spark.read.jdbc(jdbc_url, \"source2.craft_market_orders_customers\", properties=db_properties),\n",
    "        'source3_craftsmans': spark.read.jdbc(jdbc_url, \"source3.craft_market_craftsmans\", properties=db_properties),\n",
    "        'source3_customers': spark.read.jdbc(jdbc_url, \"source3.craft_market_customers\", properties=db_properties),\n",
    "        'source3_orders': spark.read.jdbc(jdbc_url, \"source3.craft_market_orders\", properties=db_properties)\n",
    "    }\n",
    "    \n",
    "    for name, df in source_tables.items():\n",
    "        log_count(df, f\"Read {name}\")\n",
    "    \n",
    "    return source_tables\n",
    "\n",
    "def process_customers(sources):\n",
    "    customers_union = sources['source1_wide'].select(\n",
    "        \"customer_id\", \"customer_name\", \"customer_address\", \n",
    "        \"customer_birthday\", \"customer_email\"\n",
    "    ).union(\n",
    "        sources['source2_orders'].select(\n",
    "            \"customer_id\", \"customer_name\", \"customer_address\", \n",
    "            \"customer_birthday\", \"customer_email\"\n",
    "        )\n",
    "    ).union(\n",
    "        sources['source3_customers'].select(\n",
    "            \"customer_id\", \"customer_name\", \"customer_address\", \n",
    "            \"customer_birthday\", \"customer_email\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    d_customers = customers_union \\\n",
    "        .withColumn(\"valid_from\", current_timestamp().cast(TimestampType())) \\\n",
    "        .withColumn(\"valid_to\", lit(None).cast(TimestampType())) \\\n",
    "        .withColumn(\"is_current\", lit(True))\n",
    "    \n",
    "    return d_customers\n",
    "\n",
    "def process_craftsmans(sources):\n",
    "    craftsmans_union = sources['source1_wide'].select(\n",
    "        \"craftsman_id\", \"craftsman_name\", \"craftsman_address\", \n",
    "        \"craftsman_birthday\", \"craftsman_email\"\n",
    "    ).union(\n",
    "        sources['source2_masters'].select(\n",
    "            \"craftsman_id\", \"craftsman_name\", \"craftsman_address\", \n",
    "            \"craftsman_birthday\", \"craftsman_email\"\n",
    "        )\n",
    "    ).union(\n",
    "        sources['source3_craftsmans'].select(\n",
    "            \"craftsman_id\", \"craftsman_name\", \"craftsman_address\", \n",
    "            \"craftsman_birthday\", \"craftsman_email\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    d_craftsmans = craftsmans_union \\\n",
    "        .withColumn(\"valid_from\", current_timestamp().cast(TimestampType())) \\\n",
    "        .withColumn(\"valid_to\", lit(None).cast(TimestampType())) \\\n",
    "        .withColumn(\"is_current\", lit(True))\n",
    "    \n",
    "    return d_craftsmans\n",
    "\n",
    "def process_products(sources):\n",
    "    def add_product_business_key(df):\n",
    "        return df.withColumn(\n",
    "            \"product_business_key\",\n",
    "            concat_ws('::', \n",
    "                col(\"product_name\"),\n",
    "                col(\"product_description\"),\n",
    "                col(\"product_type\"),\n",
    "                col(\"product_price\").cast(\"string\")\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    products_source1 = add_product_business_key(\n",
    "        sources['source1_wide'].select(\n",
    "            \"product_name\", \"product_description\", \n",
    "            \"product_type\", \"product_price\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    products_source2 = add_product_business_key(\n",
    "        sources['source2_masters'].select(\n",
    "            \"product_name\", \"product_description\", \n",
    "            \"product_type\", \"product_price\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    products_source3 = add_product_business_key(\n",
    "        sources['source3_orders'].select(\n",
    "            \"product_name\", \"product_description\", \n",
    "            \"product_type\", \"product_price\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    products_union = products_source1.union(products_source2).union(products_source3)\n",
    "    \n",
    "    print(f\"Products before deduplication: {products_union.count()}\")\n",
    "    \n",
    "    products_deduped = products_union.dropDuplicates([\"product_business_key\"])\n",
    "    \n",
    "    print(f\"Products after deduplication: {products_deduped.count()}\")\n",
    "    \n",
    "    window_spec = Window.orderBy(\"product_business_key\")\n",
    "    d_products = products_deduped \\\n",
    "        .withColumn(\"product_id\", row_number().over(window_spec)) \\\n",
    "        .withColumn(\"valid_from\", current_timestamp().cast(TimestampType())) \\\n",
    "        .withColumn(\"valid_to\", lit(None).cast(TimestampType())) \\\n",
    "        .withColumn(\"is_current\", lit(True)) \\\n",
    "        .select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"product_description\",\n",
    "            \"product_type\",\n",
    "            \"product_price\",\n",
    "            \"valid_from\",\n",
    "            \"valid_to\",\n",
    "            \"is_current\",\n",
    "            \"product_business_key\"\n",
    "        )\n",
    "    \n",
    "    return d_products\n",
    "\n",
    "def create_unique_order_key(df, source):\n",
    "    return df.withColumn(\n",
    "        \"order_key\",\n",
    "        concat_ws(':', \n",
    "            lit(source),\n",
    "            col(\"customer_id\"),\n",
    "            col(\"craftsman_id\"),\n",
    "            col(\"product_id\"),\n",
    "            coalesce(col(\"order_created_date\").cast(\"string\"), lit(\"null\")),\n",
    "            coalesce(col(\"order_completion_date\").cast(\"string\"), lit(\"null\")),\n",
    "            col(\"order_status\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "def process_orders(sources, d_customers, d_craftsmans, d_products):\n",
    "    \n",
    "    source1_orders = create_unique_order_key(\n",
    "        sources['source1_wide'].select(\n",
    "            \"customer_id\",\n",
    "            \"craftsman_id\",\n",
    "            \"product_id\",\n",
    "            \"order_created_date\",\n",
    "            \"order_completion_date\",\n",
    "            \"order_status\"\n",
    "        ),\n",
    "        \"source1\"\n",
    "    )\n",
    "    \n",
    "    source2_orders = create_unique_order_key(\n",
    "        sources['source2_orders'].select(\n",
    "            \"customer_id\",\n",
    "            \"craftsman_id\",\n",
    "            \"product_id\",\n",
    "            \"order_created_date\",\n",
    "            \"order_completion_date\",\n",
    "            \"order_status\"\n",
    "        ),\n",
    "        \"source2\"\n",
    "    )\n",
    "    \n",
    "    source3_orders = create_unique_order_key(\n",
    "        sources['source3_orders'].select(\n",
    "            \"customer_id\",\n",
    "            \"craftsman_id\",\n",
    "            \"product_id\",\n",
    "            \"order_created_date\",\n",
    "            \"order_completion_date\",\n",
    "            \"order_status\"\n",
    "        ),\n",
    "        \"source3\"\n",
    "    )\n",
    "\n",
    "    orders_union = source1_orders.union(source2_orders).union(source3_orders)\n",
    "    \n",
    "    print(f\"Total orders before deduplication: {orders_union.count()}\")\n",
    "    \n",
    "    duplicates = orders_union.groupBy(\"order_key\") \\\n",
    "        .agg(count(\"*\").alias(\"count\")) \\\n",
    "        .filter(col(\"count\") > 1)\n",
    "    \n",
    "    print(f\"Number of duplicated order keys: {duplicates.count()}\")\n",
    "    if duplicates.count() > 0:\n",
    "        print(\"Sample of duplicates:\")\n",
    "        duplicates.show(5, truncate=False)\n",
    "    \n",
    "    orders_deduplicated = orders_union.dropDuplicates([\"order_key\"])\n",
    "    \n",
    "    print(f\"Total orders after deduplication: {orders_deduplicated.count()}\")\n",
    "    \n",
    "    window_spec = Window.orderBy(\"order_created_date\", \"order_key\")\n",
    "    orders_with_id = orders_deduplicated.withColumn(\n",
    "        \"order_id\",\n",
    "        row_number().over(window_spec)\n",
    "    )\n",
    "    \n",
    "    def check_duplicates(df, key_column, name):\n",
    "        dups = df.groupBy(key_column).count().filter(col(\"count\") > 1)\n",
    "        if dups.count() > 0:\n",
    "            print(f\"Duplicates in {name} by {key_column}:\")\n",
    "            dups.show()\n",
    "        return dups.count()\n",
    "    \n",
    "    f_orders = orders_with_id \\\n",
    "        .join(\n",
    "            d_customers.dropDuplicates([\"customer_id\"]),\n",
    "            [\"customer_id\"],\n",
    "            \"left\"\n",
    "        ) \\\n",
    "        .join(\n",
    "            d_craftsmans.dropDuplicates([\"craftsman_id\"]),\n",
    "            [\"craftsman_id\"],\n",
    "            \"left\"\n",
    "        ) \\\n",
    "        .join(\n",
    "            d_products.dropDuplicates([\"product_id\"]),\n",
    "            [\"product_id\"],\n",
    "            \"left\"\n",
    "        ) \\\n",
    "        .select(\n",
    "            orders_with_id.order_id,\n",
    "            orders_with_id.customer_id,\n",
    "            orders_with_id.craftsman_id,\n",
    "            orders_with_id.product_id,\n",
    "            orders_with_id.order_created_date,\n",
    "            orders_with_id.order_completion_date,\n",
    "            orders_with_id.order_status,\n",
    "            current_timestamp().cast(TimestampType()).alias(\"load_dttm\"),\n",
    "            orders_with_id.order_key.alias(\"source_system_id\")\n",
    "        )\n",
    "    \n",
    "    print(f\"Final orders count in f_orders: {f_orders.count()}\")\n",
    "    \n",
    "    return f_orders\n",
    "\n",
    "def check_for_changes(new_df, table_name, jdbc_url, db_properties):\n",
    "    try:\n",
    "        spark = new_df.sparkSession\n",
    "        existing_df = spark.read.jdbc(\n",
    "            jdbc_url,\n",
    "            f\"dwh.{table_name}\",\n",
    "            properties=db_properties\n",
    "        )\n",
    "        \n",
    "        if table_name.startswith('d_'):\n",
    "            business_columns = [\n",
    "                c for c in new_df.columns\n",
    "                if c not in ['valid_from', 'valid_to', 'is_current']\n",
    "            ]\n",
    "            \n",
    "            new_business_data = new_df.select(business_columns)\n",
    "            existing_business_data = existing_df.select(business_columns)\n",
    "            \n",
    "            diff_count = new_business_data.exceptAll(existing_business_data).count()\n",
    "            \n",
    "        else:  # For fact table\n",
    "            compare_columns = [c for c in new_df.columns if c != 'load_dttm']\n",
    "            \n",
    "            new_compare_data = new_df.select(compare_columns)\n",
    "            existing_compare_data = existing_df.select(compare_columns)\n",
    "            \n",
    "            diff_count = new_compare_data.exceptAll(existing_compare_data).count()\n",
    "        \n",
    "        print(f\"Found {diff_count} different records for {table_name}\")\n",
    "        return diff_count > 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"No existing data found for {table_name} or error occurred: {str(e)}\")\n",
    "        return True\n",
    "\n",
    "def write_to_dwh(df, table_name, jdbc_url, db_properties):\n",
    "    \"\"\"Write dataframe to DWH\"\"\"\n",
    "    df.write \\\n",
    "        .jdbc(\n",
    "            jdbc_url,\n",
    "            f\"dwh.{table_name}\",\n",
    "            mode=\"overwrite\",\n",
    "            properties=db_properties\n",
    "        )\n",
    "    print(f\"Written {df.count()} records to dwh.{table_name}\")\n",
    "\n",
    "def upsert_dimension_table(new_df, table_name, business_key_cols, jdbc_url, db_properties):\n",
    "    spark = new_df.sparkSession\n",
    "    \n",
    "    try:\n",
    "        existing_df = spark.read.jdbc(\n",
    "            jdbc_url,\n",
    "            f\"dwh.{table_name}\",\n",
    "            properties=db_properties\n",
    "        )\n",
    "    except:\n",
    "        print(f\"Dimension table dwh.{table_name} not found. Creating a new one.\")\n",
    "        new_df.write.jdbc(\n",
    "            jdbc_url,\n",
    "            f\"dwh.{table_name}\",\n",
    "            mode=\"append\",\n",
    "            properties=db_properties\n",
    "        )\n",
    "        print(f\"Inserted {new_df.count()} rows into dwh.{table_name}\")\n",
    "        return\n",
    "    \n",
    "    existing_current = existing_df.filter(col(\"is_current\") == True)\n",
    "\n",
    "    exclude_cols = {\"valid_from\", \"valid_to\", \"is_current\"}\n",
    "    compare_cols = [c for c in new_df.columns if c not in exclude_cols]\n",
    "    \n",
    "    print(f\"\\nComparing columns: {compare_cols}\")\n",
    "    print(f\"Business key columns: {business_key_cols}\")\n",
    "    \n",
    "    join_cond = [new_df[k].eqNullSafe(existing_current[k]) for k in business_key_cols]\n",
    "    joined = new_df.alias(\"n\").join(\n",
    "        existing_current.alias(\"e\"),\n",
    "        join_cond,\n",
    "        \"left\"\n",
    "    )\n",
    "    \n",
    "    change_condition = \" OR \".join([\n",
    "        f\"NOT (n.{c} IS NOT DISTINCT FROM e.{c})\"\n",
    "        for c in compare_cols\n",
    "    ])\n",
    "    \n",
    "    changed_or_new = joined.filter(expr(change_condition))\n",
    "    print(f\"\\nFound {changed_or_new.count()} changed or new records\")\n",
    "    \n",
    "    if changed_or_new.count() > 0:\n",
    "        print(\"Sample of changes:\")\n",
    "        select_cols = []\n",
    "        for c in compare_cols:\n",
    "            select_cols.append(col(f\"n.{c}\").alias(f\"new_{c}\"))\n",
    "            select_cols.append(col(f\"e.{c}\").alias(f\"old_{c}\"))\n",
    "        \n",
    "        changed_or_new.select(select_cols).show(5, truncate=False)\n",
    "    \n",
    "        to_close = changed_or_new.filter(col(\"e.\"+business_key_cols[0]).isNotNull()) \\\n",
    "            .selectExpr(\"e.*\")\n",
    "        \n",
    "        if to_close.count() > 0:\n",
    "            print(f\"\\nClosing {to_close.count()} existing records\")\n",
    "            \n",
    "            to_close_updates = to_close \\\n",
    "                .withColumn(\"valid_to\", current_timestamp()) \\\n",
    "                .withColumn(\"is_current\", lit(False))\n",
    "            \n",
    "            new_rows = changed_or_new.selectExpr(\"n.*\") \\\n",
    "                .withColumn(\"valid_from\", current_timestamp()) \\\n",
    "                .withColumn(\"valid_to\", lit(None).cast(TimestampType())) \\\n",
    "                .withColumn(\"is_current\", lit(True))\n",
    "            \n",
    "            closed_keys = to_close.select(*business_key_cols).distinct()\n",
    "            \n",
    "            unchanged_current = existing_current.join(\n",
    "                closed_keys,\n",
    "                on=business_key_cols,\n",
    "                how=\"leftanti\"\n",
    "            )\n",
    "            \n",
    "            existing_history = existing_df.filter(col(\"is_current\") == False)\n",
    "            \n",
    "            final_dim = unchanged_current.unionByName(existing_history) \\\n",
    "                .unionByName(to_close_updates) \\\n",
    "                .unionByName(new_rows)\n",
    "            \n",
    "            # Write back to database\n",
    "            final_dim.write.jdbc(\n",
    "                jdbc_url,\n",
    "                f\"dwh.{table_name}\",\n",
    "                mode=\"overwrite\",\n",
    "                properties=db_properties\n",
    "            )\n",
    "            \n",
    "            print(f\"Upserted (closed + new) {to_close_updates.count() + new_rows.count()} rows in dwh.{table_name}\")\n",
    "            \n",
    "        else:\n",
    "            new_inserts = changed_or_new.selectExpr(\"n.*\") \\\n",
    "                .withColumn(\"valid_from\", current_timestamp()) \\\n",
    "                .withColumn(\"valid_to\", lit(None).cast(TimestampType())) \\\n",
    "                .withColumn(\"is_current\", lit(True))\n",
    "            \n",
    "            if new_inserts.count() > 0:\n",
    "                final_dim = existing_df.unionByName(new_inserts)\n",
    "                final_dim.write.jdbc(\n",
    "                    jdbc_url,\n",
    "                    f\"dwh.{table_name}\",\n",
    "                    mode=\"overwrite\",\n",
    "                    properties=db_properties\n",
    "                )\n",
    "                print(f\"Inserted {new_inserts.count()} new rows in dwh.{table_name}\")\n",
    "    else:\n",
    "        print(f\"No new or changed rows for dwh.{table_name}. No action taken.\")\n",
    "\n",
    "def upsert_fact_table(new_facts, table_name, unique_key_col, jdbc_url, db_properties):\n",
    "    spark = new_facts.sparkSession\n",
    "    \n",
    "    try:\n",
    "        existing_facts = spark.read.jdbc(\n",
    "            jdbc_url,\n",
    "            f\"dwh.{table_name}\",\n",
    "            properties=db_properties\n",
    "        )\n",
    "    except:\n",
    "        print(f\"Fact table dwh.{table_name} not found. Creating a new one.\")\n",
    "        new_facts.write.jdbc(\n",
    "            jdbc_url,\n",
    "            f\"dwh.{table_name}\",\n",
    "            mode=\"append\",\n",
    "            properties=db_properties\n",
    "        )\n",
    "        print(f\"Inserted {new_facts.count()} rows into dwh.{table_name}\")\n",
    "        return\n",
    "    \n",
    "    join_cond = [new_facts[unique_key_col] == existing_facts[unique_key_col]]\n",
    "    joined = new_facts.alias(\"n\").join(existing_facts.alias(\"e\"), on=join_cond, how=\"left\")\n",
    "    \n",
    "    new_only = joined.filter(col(f\"e.{unique_key_col}\").isNull()).selectExpr(\"n.*\")\n",
    "    \n",
    "    if new_only.count() > 0:\n",
    "        new_only.write.jdbc(\n",
    "            jdbc_url,\n",
    "            f\"dwh.{table_name}\",\n",
    "            mode=\"append\",\n",
    "            properties=db_properties\n",
    "        )\n",
    "        print(f\"Inserted {new_only.count()} new fact rows into dwh.{table_name}\")\n",
    "    else:\n",
    "        print(f\"No new rows found for dwh.{table_name}. No action taken.\")\n",
    "\n",
    "def load_data_into_warehouse():\n",
    "    spark = init_spark()\n",
    "    \n",
    "    jdbc_url = \"jdbc:postgresql://spark_db:5432/db\"\n",
    "    db_properties = {\n",
    "        \"user\": \"user\",\n",
    "        \"password\": \"password\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        sources = read_source_tables(spark, jdbc_url, db_properties)\n",
    "        \n",
    "        print(\"Processing dimension tables...\")\n",
    "        d_customers = process_customers(sources)\n",
    "        d_craftsmans = process_craftsmans(sources)\n",
    "        d_products = process_products(sources)\n",
    "        \n",
    "        print(\"Processing fact table...\")\n",
    "        f_orders = process_orders(sources, d_customers, d_craftsmans, d_products)\n",
    "        \n",
    "        print(\"Upserting dimension tables to DWH...\")\n",
    "        upsert_dimension_table(\n",
    "            d_customers,\n",
    "            \"d_customers\",\n",
    "            business_key_cols=[\"customer_email\"],\n",
    "            jdbc_url=jdbc_url,\n",
    "            db_properties=db_properties\n",
    "        )\n",
    "        \n",
    "        upsert_dimension_table(\n",
    "            d_craftsmans,\n",
    "            \"d_craftsmans\",\n",
    "            business_key_cols=[\"craftsman_email\"],\n",
    "            jdbc_url=jdbc_url,\n",
    "            db_properties=db_properties\n",
    "        )\n",
    "        \n",
    "        upsert_dimension_table(\n",
    "            d_products,\n",
    "            \"d_products\",\n",
    "            business_key_cols=[\"product_business_key\"],\n",
    "            jdbc_url=jdbc_url,\n",
    "            db_properties=db_properties\n",
    "        )\n",
    "        \n",
    "        print(\"Upserting fact table to DWH...\")\n",
    "        upsert_fact_table(\n",
    "            f_orders,\n",
    "            \"f_orders\",\n",
    "            unique_key_col=\"source_system_id\", \n",
    "            jdbc_url=jdbc_url,\n",
    "            db_properties=db_properties\n",
    "        )\n",
    "        \n",
    "        print(\"DWH load completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during DWH load: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "load_data_into_warehouse()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5f1312-fcc8-40fb-90ba-9b55b9f9a332",
   "metadata": {},
   "source": [
    "Ура, загрузили все данные. По их числу видно что загрузились правильно. Теперь можно проверить что без изменений оно ничего добавлять не будет: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "496221d9-063c-4324-ad5f-322aae4c5a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read source1_wide: 999\n",
      "Read source2_masters: 999\n",
      "Read source2_orders: 999\n",
      "Read source3_craftsmans: 999\n",
      "Read source3_customers: 999\n",
      "Read source3_orders: 999\n",
      "Processing dimension tables...\n",
      "Products before deduplication: 2997\n",
      "Products after deduplication: 2994\n",
      "Processing fact table...\n",
      "Total orders before deduplication: 2997\n",
      "Number of duplicated order keys: 0\n",
      "Total orders after deduplication: 2997\n",
      "Final orders count in f_orders: 2997\n",
      "Upserting dimension tables to DWH...\n",
      "\n",
      "Comparing columns: ['customer_id', 'customer_name', 'customer_address', 'customer_birthday', 'customer_email']\n",
      "Business key columns: ['customer_email']\n",
      "\n",
      "Found 0 changed or new records\n",
      "No new or changed rows for dwh.d_customers. No action taken.\n",
      "\n",
      "Comparing columns: ['craftsman_id', 'craftsman_name', 'craftsman_address', 'craftsman_birthday', 'craftsman_email']\n",
      "Business key columns: ['craftsman_email']\n",
      "\n",
      "Found 0 changed or new records\n",
      "No new or changed rows for dwh.d_craftsmans. No action taken.\n",
      "\n",
      "Comparing columns: ['product_id', 'product_name', 'product_description', 'product_type', 'product_price', 'product_business_key']\n",
      "Business key columns: ['product_business_key']\n",
      "\n",
      "Found 0 changed or new records\n",
      "No new or changed rows for dwh.d_products. No action taken.\n",
      "Upserting fact table to DWH...\n",
      "No new rows found for dwh.f_orders. No action taken.\n",
      "DWH load completed successfully (incremental)!\n"
     ]
    }
   ],
   "source": [
    "load_data_into_warehouse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4945737-4b18-4093-b576-576b94bc0519",
   "metadata": {},
   "source": [
    "Теперь обновим данные в витрине"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "583d226b-8a83-4c99-bfa6-307566a5b6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found last load date in control table: 2024-12-26 04:20:54.469924\n",
      "Last load date: 2024-12-26 04:20:54.469924\n",
      "No periods need recalculation\n",
      "Read from DWH:\n",
      "    - 2997 craftsmen records\n",
      "    - 2997 customer records\n",
      "    - 2994 product records\n",
      "    - 2997 order records\n",
      "Calculating metrics...\n",
      "Updating datamart...\n",
      "No periods need recalculation, skipping update\n",
      "Datamart update completed successfully!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def init_spark():\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"Craftsman Report Datamart\") \\\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def get_last_load_date(spark, jdbc_url, properties):\n",
    "    try:\n",
    "        control_table_query = \"\"\"\n",
    "        (SELECT load_dttm \n",
    "         FROM dwh.load_dates_craftsman_report_datamart \n",
    "         ORDER BY load_dttm DESC \n",
    "         LIMIT 1) AS last_load\n",
    "        \"\"\"\n",
    "        \n",
    "        df = spark.read.jdbc(\n",
    "            jdbc_url,\n",
    "            control_table_query,\n",
    "            properties=properties\n",
    "        )\n",
    "        \n",
    "        if df.count() > 0:\n",
    "            last_load = df.collect()[0]['load_dttm']\n",
    "            print(f\"Found last load date in control table: {last_load}\")\n",
    "            return last_load\n",
    "        else:\n",
    "            print(\"No previous load date found in control table\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting last load date: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def read_dwh_data(spark, jdbc_url, properties, last_load_date):\n",
    "    \n",
    "    d_craftsmans = spark.read.jdbc(\n",
    "        jdbc_url, \n",
    "        \"(SELECT * FROM dwh.d_craftsmans WHERE is_current = true) AS d_craftsmans\", \n",
    "        properties=properties\n",
    "    )\n",
    "    \n",
    "    d_customers = spark.read.jdbc(\n",
    "        jdbc_url, \n",
    "        \"(SELECT * FROM dwh.d_customers WHERE is_current = true) AS d_customers\", \n",
    "        properties=properties\n",
    "    )\n",
    "    \n",
    "    d_products = spark.read.jdbc(\n",
    "        jdbc_url, \n",
    "        \"(SELECT * FROM dwh.d_products WHERE is_current = true) AS d_products\", \n",
    "        properties=properties\n",
    "    )\n",
    "    \n",
    "    if last_load_date:\n",
    "        orders_query = f\"\"\"\n",
    "        (SELECT * FROM dwh.f_orders \n",
    "         WHERE DATE(load_dttm) >= DATE('{last_load_date}')\n",
    "         OR DATE(order_created_date) >= DATE('{last_load_date}')\n",
    "         OR DATE(order_completion_date) >= DATE('{last_load_date}')\n",
    "        ) AS f_orders\n",
    "        \"\"\"\n",
    "    else:\n",
    "        orders_query = \"dwh.f_orders\"\n",
    "    \n",
    "    f_orders = spark.read.jdbc(jdbc_url, orders_query, properties=properties)\n",
    "    \n",
    "    print(f\"\"\"Read from DWH:\n",
    "    - {d_craftsmans.count()} craftsmen records\n",
    "    - {d_customers.count()} customer records\n",
    "    - {d_products.count()} product records\n",
    "    - {f_orders.count()} order records\"\"\")\n",
    "    \n",
    "    return d_craftsmans, d_customers, d_products, f_orders\n",
    "\n",
    "def get_existing_periods(spark, jdbc_url, properties, last_load_date):\n",
    "    if last_load_date:\n",
    "        query = f\"\"\"\n",
    "        (SELECT DISTINCT report_period \n",
    "         FROM dwh.craftsman_report_datamart \n",
    "         WHERE report_period >= '{last_load_date.strftime('%Y-%m')}') AS periods\n",
    "        \"\"\"\n",
    "        return spark.read.jdbc(jdbc_url, query, properties=properties) \\\n",
    "            .select(\"report_period\") \\\n",
    "            .collect()\n",
    "    return []\n",
    "\n",
    "def calculate_metrics(d_craftsmans, d_customers, d_products, f_orders):\n",
    "    \n",
    "    orders_with_period = f_orders.withColumn(\n",
    "        \"report_period\",\n",
    "        date_format(col(\"order_created_date\"), \"yyyy-MM\")\n",
    "    )\n",
    "    \n",
    "    enriched_orders = orders_with_period \\\n",
    "        .join(broadcast(d_craftsmans), \"craftsman_id\") \\\n",
    "        .join(broadcast(d_customers), \"customer_id\") \\\n",
    "        .join(broadcast(d_products), \"product_id\")\n",
    "    \n",
    "    enriched_orders = enriched_orders.withColumn(\n",
    "        \"customer_age\",\n",
    "        floor(datediff(col(\"order_created_date\"), col(\"customer_birthday\")) / 365.25)\n",
    "    )\n",
    "    \n",
    "    enriched_orders = enriched_orders.withColumn(\n",
    "        \"completion_time\",\n",
    "        when(\n",
    "            col(\"order_completion_date\").isNotNull(),\n",
    "            datediff(col(\"order_completion_date\"), col(\"order_created_date\"))\n",
    "        ).otherwise(None)\n",
    "    )\n",
    "    \n",
    "    window_spec = Window.partitionBy(\"craftsman_id\", \"report_period\")\n",
    "    \n",
    "    metrics = enriched_orders.groupBy(\"craftsman_id\", \"report_period\") \\\n",
    "        .agg(\n",
    "            first(\"craftsman_name\").alias(\"craftsman_name\"),\n",
    "            first(\"craftsman_address\").alias(\"craftsman_address\"),\n",
    "            first(\"craftsman_birthday\").alias(\"craftsman_birthday\"),\n",
    "            first(\"craftsman_email\").alias(\"craftsman_email\"),\n",
    "            \n",
    "            round(sum(\"product_price\") * 0.9, 2).alias(\"craftsman_money\"),\n",
    "            (sum(\"product_price\") * 0.1).cast(\"bigint\").alias(\"platform_money\"),\n",
    "            \n",
    "            count(\"order_id\").alias(\"count_order\"),\n",
    "            round(avg(\"product_price\"), 2).alias(\"avg_price_order\"),\n",
    "            \n",
    "            round(avg(\"customer_age\"), 1).alias(\"avg_age_customer\"),\n",
    "            \n",
    "            expr(\"percentile_approx(completion_time, 0.5)\").alias(\"median_time_order_completed\"),\n",
    "            \n",
    "            sum(when(col(\"order_status\") == \"created\", 1).otherwise(0)).alias(\"count_order_created\"),\n",
    "            sum(when(col(\"order_status\") == \"in progress\", 1).otherwise(0)).alias(\"count_order_in_progress\"),\n",
    "            sum(when(col(\"order_status\") == \"delivery\", 1).otherwise(0)).alias(\"count_order_delivery\"),\n",
    "            sum(when(col(\"order_status\") == \"done\", 1).otherwise(0)).alias(\"count_order_done\"),\n",
    "            sum(when(col(\"order_status\") != \"done\", 1).otherwise(0)).alias(\"count_order_not_done\")\n",
    "        )\n",
    "    \n",
    "    top_categories = enriched_orders \\\n",
    "        .groupBy(\"craftsman_id\", \"report_period\", \"product_type\") \\\n",
    "        .count() \\\n",
    "        .withColumn(\n",
    "            \"rank\",\n",
    "            row_number().over(\n",
    "                Window.partitionBy(\"craftsman_id\", \"report_period\")\n",
    "                .orderBy(desc(\"count\"))\n",
    "            )\n",
    "        ) \\\n",
    "        .filter(col(\"rank\") == 1) \\\n",
    "        .select(\"craftsman_id\", \"report_period\", \"product_type\")\n",
    "    \n",
    "    # Объединяем все метрики\n",
    "    final_metrics = metrics.join(\n",
    "        top_categories,\n",
    "        [\"craftsman_id\", \"report_period\"]\n",
    "    ).withColumnRenamed(\"product_type\", \"top_product_category\")\n",
    "    \n",
    "    return final_metrics\n",
    "\n",
    "def get_periods_to_recalculate(spark, jdbc_url, properties, last_load_date):\n",
    "    affected_periods = set()  \n",
    "\n",
    "    if not last_load_date:\n",
    "        all_periods_query = \"\"\"\n",
    "        SELECT DISTINCT DATE_TRUNC('month', order_created_date)::date as report_period\n",
    "        FROM dwh.f_orders\n",
    "        ORDER BY report_period\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            all_periods = spark.read.jdbc(\n",
    "                jdbc_url,\n",
    "                f\"({all_periods_query}) AS periods\",\n",
    "                properties=properties\n",
    "            ).collect()\n",
    "            \n",
    "            affected_periods.update(row.report_period for row in all_periods)\n",
    "            print(\"First load - will process all periods:\")\n",
    "            print(f\"Found {len(affected_periods)} periods to process\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting all periods: {str(e)}\")\n",
    "            return []\n",
    "    else:\n",
    "        dim_tables = [\n",
    "            {\n",
    "                'table': 'dwh.d_customers',\n",
    "                'key': 'customer_id',\n",
    "                'name': 'customers'\n",
    "            },\n",
    "            {\n",
    "                'table': 'dwh.d_craftsmans',\n",
    "                'key': 'craftsman_id',\n",
    "                'name': 'craftsmen'\n",
    "            },\n",
    "            {\n",
    "                'table': 'dwh.d_products',\n",
    "                'key': 'product_id',\n",
    "                'name': 'products'\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        for dim in dim_tables:\n",
    "            dim_changes_query = f\"\"\"\n",
    "            WITH changed_dim AS (\n",
    "                SELECT {dim['key']}\n",
    "                FROM {dim['table']}\n",
    "                WHERE valid_from >= '{last_load_date}'\n",
    "                    OR valid_to >= '{last_load_date}'\n",
    "            )\n",
    "            SELECT DISTINCT \n",
    "                DATE_TRUNC('month', f.order_created_date)::date as report_period\n",
    "            FROM dwh.f_orders f\n",
    "            INNER JOIN changed_dim cd ON f.{dim['key']} = cd.{dim['key']}\n",
    "            \"\"\"\n",
    "            \n",
    "            try:\n",
    "                changed_periods = spark.read.jdbc(\n",
    "                    jdbc_url,\n",
    "                    f\"({dim_changes_query}) AS periods\",\n",
    "                    properties=properties\n",
    "                ).collect()\n",
    "                \n",
    "                if changed_periods:\n",
    "                    periods = {row.report_period for row in changed_periods}\n",
    "                    affected_periods.update(periods)\n",
    "                    print(f\"Found periods to recalculate due to {dim['name']} changes: {sorted(periods)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error checking {dim['name']} changes: {str(e)}\")\n",
    "\n",
    "        if not affected_periods:\n",
    "            new_facts_query = f\"\"\"\n",
    "            SELECT DISTINCT DATE_TRUNC('month', order_created_date)::date as report_period\n",
    "            FROM dwh.f_orders\n",
    "            WHERE load_dttm >= '{last_load_date}'\n",
    "            \"\"\"\n",
    "            \n",
    "            try:\n",
    "                new_fact_periods = spark.read.jdbc(\n",
    "                    jdbc_url,\n",
    "                    f\"({new_facts_query}) AS periods\",\n",
    "                    properties=properties\n",
    "                ).collect()\n",
    "                \n",
    "                if new_fact_periods:\n",
    "                    fact_periods = {row.report_period for row in new_fact_periods}\n",
    "                    affected_periods.update(fact_periods)\n",
    "                    print(f\"Found periods to recalculate due to new fact records: {sorted(fact_periods)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error checking new fact records: {str(e)}\")\n",
    "\n",
    "    periods_list = sorted(list(affected_periods))\n",
    "    \n",
    "    if periods_list:\n",
    "        print(f\"Total unique periods to recalculate: {periods_list}\")\n",
    "        print(f\"Number of periods to recalculate: {len(periods_list)}\")\n",
    "    else:\n",
    "        print(\"No periods need recalculation\")\n",
    "    \n",
    "    return periods_list\n",
    "\n",
    "def update_datamart(spark, df, jdbc_url, properties, periods_to_recalc):\n",
    "    if not periods_to_recalc:\n",
    "        print(\"No periods need recalculation, skipping update\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        current_data = spark.read.jdbc(\n",
    "            jdbc_url,\n",
    "            \"dwh.craftsman_report_datamart\",\n",
    "            properties=properties\n",
    "        )\n",
    "\n",
    "        period_strings = [period.strftime('%Y-%m') for period in periods_to_recalc]\n",
    "        print(f\"Updating periods: {period_strings}\")\n",
    "\n",
    "        current_filtered = current_data.filter(\n",
    "            ~col(\"report_period\").isin(period_strings)\n",
    "        )\n",
    "\n",
    "        print(f\"Records retained after filtering: {current_filtered.count()}\")\n",
    "\n",
    "        if current_filtered.count() > 0:\n",
    "            new_data = df.filter(col(\"report_period\").isin(period_strings))\n",
    "            print(f\"New records for affected periods: {new_data.count()}\")\n",
    "            \n",
    "            final_data = current_filtered.unionByName(new_data)\n",
    "        else:\n",
    "            final_data = df\n",
    "\n",
    "        print(f\"Total records to write: {final_data.count()}\")\n",
    "\n",
    "        final_data.write.jdbc(\n",
    "            jdbc_url,\n",
    "            \"dwh.craftsman_report_datamart\",\n",
    "            mode=\"overwrite\",\n",
    "            properties=properties\n",
    "        )\n",
    "        print(f\"Written {final_data.count()} records to datamart\")\n",
    "\n",
    "        current_timestamp = datetime.now()\n",
    "        spark.createDataFrame(\n",
    "            [(current_timestamp,)],\n",
    "            [\"load_dttm\"]\n",
    "        ).write.jdbc(\n",
    "            jdbc_url,\n",
    "            \"dwh.load_dates_craftsman_report_datamart\",\n",
    "            mode=\"append\",\n",
    "            properties=properties\n",
    "        )\n",
    "        print(f\"Updated control table with load timestamp: {current_timestamp}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating datamart: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating datamart: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def load_data_into_datamart():\n",
    "    spark = init_spark()\n",
    "    \n",
    "    jdbc_url = \"jdbc:postgresql://spark_db:5432/db\"\n",
    "    properties = {\n",
    "        \"user\": \"user\",\n",
    "        \"password\": \"password\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        last_load_date = get_last_load_date(spark, jdbc_url, properties)\n",
    "        print(f\"Last load date: {last_load_date}\")\n",
    "        \n",
    "        periods_to_recalc = get_periods_to_recalculate(\n",
    "            spark, jdbc_url, properties, last_load_date\n",
    "        )\n",
    "        \n",
    "        d_craftsmans, d_customers, d_products, f_orders = read_dwh_data(\n",
    "            spark, jdbc_url, properties, last_load_date\n",
    "        )\n",
    "        \n",
    "        print(\"Calculating metrics...\")\n",
    "        final_metrics = calculate_metrics(\n",
    "            d_craftsmans, d_customers, d_products, f_orders\n",
    "        )\n",
    "        \n",
    "        print(\"Updating datamart...\")\n",
    "        update_datamart(\n",
    "            spark, final_metrics, jdbc_url, properties, periods_to_recalc\n",
    "        )\n",
    "        \n",
    "        print(\"Datamart update completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during datamart update: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "load_data_into_datamart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8df0b240-5b07-4419-aff1-bd053cf048af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found last load date in control table: 2024-12-26 04:20:54.469924\n",
      "Last load date: 2024-12-26 04:20:54.469924\n",
      "No periods need recalculation\n",
      "Read from DWH:\n",
      "    - 2997 craftsmen records\n",
      "    - 2997 customer records\n",
      "    - 2994 product records\n",
      "    - 2997 order records\n",
      "Calculating metrics...\n",
      "Updating datamart...\n",
      "No periods need recalculation, skipping update\n",
      "Datamart update completed successfully!\n"
     ]
    }
   ],
   "source": [
    "load_data_into_datamart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15e5a14-2ec9-488c-8d26-9ac4b9a3d25c",
   "metadata": {},
   "source": [
    "Да, не обновляется без изменений. Теперь, нужно проверить, обновляется если изменение таки есть. При этом нам нужно проверить весь флоу - что оно обновится в DWH, а потом и в витрине. То есть нам нужно сделать изменение прям в оригиналах - в sources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b5b6428a-9a21-4a66-a859-1a193d9e569d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (4.9 kB)\n",
      "Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.10\n"
     ]
    }
   ],
   "source": [
    "!pip3 install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "2d057ab9-5400-4c89-9494-0acefb96113f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего customer'ов в таблице: 999\n",
      "Случайно выбран customer_id = 994\n",
      "Старое имя = 'Selia Longcake'\n",
      "Обновляем имя 'Selia Longcake' -> 'Selia Longcake_CHANGED' у customer_id = 994\n",
      "Обновлено строк: 1\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from datetime import datetime\n",
    "from random import randint\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "def init_spark():\n",
    "    \"\"\"Инициализация Spark-сессии\"\"\"\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"Update Customer with psycopg2\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def choose_random_customer(spark, jdbc_url, db_properties):\n",
    "    customers_df = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"source3.craft_market_customers\",\n",
    "        properties=db_properties\n",
    "    )\n",
    "    total_count = customers_df.count()\n",
    "    print(f\"Всего customer'ов в таблице: {total_count}\")\n",
    "\n",
    "    if total_count == 0:\n",
    "        print(\"Таблица пустая, нет записей для обновления.\")\n",
    "        return None, None \n",
    "\n",
    "    random_row = customers_df.orderBy(rand()).limit(1).collect()[0]\n",
    "\n",
    "    if \"customer_id\" in random_row:\n",
    "        customer_id = random_row[\"customer_id\"]\n",
    "        old_name = random_row[\"customer_name\"]\n",
    "        print(f\"Случайно выбран customer_id = {customer_id}\")\n",
    "        print(f\"Старое имя = '{old_name}'\")\n",
    "        return customer_id, old_name\n",
    "    else:\n",
    "        customer_email = random_row[\"customer_email\"]\n",
    "        old_name = random_row[\"customer_name\"]\n",
    "        print(f\"Случайно выбран customer_email = {customer_email}\")\n",
    "        return customer_email, old_name\n",
    "\n",
    "def update_customer_name_psycopg2(customer_id, old_name, new_name, db_host, db_port, db_name, db_user, db_password):\n",
    "    if customer_id is None:\n",
    "        print(\"Нет customer_id — нечего обновлять через psycopg2.\")\n",
    "        return\n",
    "\n",
    "    conn = psycopg2.connect(\n",
    "        database=db_name,\n",
    "        user=db_user,\n",
    "        password=db_password,\n",
    "        host=db_host,\n",
    "        port=db_port\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        sql_update = \"\"\"UPDATE source3.craft_market_customers\n",
    "                        SET customer_name = %s\n",
    "                        WHERE customer_id = %s\"\"\"\n",
    "\n",
    "        print(f\"Обновляем имя '{old_name}' -> '{new_name}' у customer_id = {customer_id}\")\n",
    "\n",
    "        cur.execute(sql_update, (new_name, customer_id))\n",
    "        conn.commit()\n",
    "\n",
    "        rows_updated = cur.rowcount\n",
    "        print(f\"Обновлено строк: {rows_updated}\")\n",
    "\n",
    "        cur.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка в процессе UPDATE через psycopg2: {str(e)}\")\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def main():\n",
    "    spark = init_spark()\n",
    "\n",
    "    jdbc_url = \"jdbc:postgresql://spark_db:5432/db\"\n",
    "    db_properties = {\n",
    "        \"user\": \"user\",\n",
    "        \"password\": \"password\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "    db_host = \"spark_db\"\n",
    "    db_port = \"5432\"\n",
    "    db_name = \"db\"\n",
    "    db_user = \"user\"\n",
    "    db_password = \"password\"\n",
    "\n",
    "    try:\n",
    "        customer_id, old_name = choose_random_customer(spark, jdbc_url, db_properties)\n",
    "        if customer_id is None:\n",
    "            print(\"Нет данных для обновления (таблица пустая). Завершение.\")\n",
    "            return\n",
    "\n",
    "        new_name = old_name + \"_CHANGED\"\n",
    "        \n",
    "        update_customer_name_psycopg2(\n",
    "            customer_id=customer_id,\n",
    "            old_name=old_name,\n",
    "            new_name=new_name,\n",
    "            db_host=db_host,\n",
    "            db_port=db_port,\n",
    "            db_name=db_name,\n",
    "            db_user=db_user,\n",
    "            db_password=db_password\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка во время операции: {str(e)}\")\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d063f5b6-0d31-4318-ab2b-efd59d528b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read source1_wide: 999\n",
      "Read source2_masters: 999\n",
      "Read source2_orders: 999\n",
      "Read source3_craftsmans: 999\n",
      "Read source3_customers: 999\n",
      "Read source3_orders: 999\n",
      "Processing dimension tables...\n",
      "Products before deduplication: 2997\n",
      "Products after deduplication: 2994\n",
      "Processing fact table...\n",
      "Total orders before deduplication: 2997\n",
      "Number of duplicated order keys: 0\n",
      "Total orders after deduplication: 2997\n",
      "Final orders count in f_orders: 2997\n",
      "Upserting dimension tables to DWH...\n",
      "\n",
      "Comparing columns: ['customer_id', 'customer_name', 'customer_address', 'customer_birthday', 'customer_email']\n",
      "Business key columns: ['customer_email']\n",
      "\n",
      "Found 1 changed or new records\n",
      "Sample of changes:\n",
      "+---------------+---------------+----------------------+-----------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+-----------------------+\n",
      "|new_customer_id|old_customer_id|new_customer_name     |old_customer_name|new_customer_address     |old_customer_address     |new_customer_birthday|old_customer_birthday|new_customer_email     |old_customer_email     |\n",
      "+---------------+---------------+----------------------+-----------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+-----------------------+\n",
      "|994            |994            |Selia Longcake_CHANGED|Selia Longcake   |629 Hollow Ridge Junction|629 Hollow Ridge Junction|1996-02-03           |1996-02-03           |slongcakerm@nytimes.com|slongcakerm@nytimes.com|\n",
      "+---------------+---------------+----------------------+-----------------+-------------------------+-------------------------+---------------------+---------------------+-----------------------+-----------------------+\n",
      "\n",
      "\n",
      "Closing 1 existing records\n",
      "Upserted (closed + new) 0 rows in dwh.d_customers\n",
      "\n",
      "Comparing columns: ['craftsman_id', 'craftsman_name', 'craftsman_address', 'craftsman_birthday', 'craftsman_email']\n",
      "Business key columns: ['craftsman_email']\n",
      "\n",
      "Found 0 changed or new records\n",
      "No new or changed rows for dwh.d_craftsmans. No action taken.\n",
      "\n",
      "Comparing columns: ['product_id', 'product_name', 'product_description', 'product_type', 'product_price', 'product_business_key']\n",
      "Business key columns: ['product_business_key']\n",
      "\n",
      "Found 0 changed or new records\n",
      "No new or changed rows for dwh.d_products. No action taken.\n",
      "Upserting fact table to DWH...\n",
      "No new rows found for dwh.f_orders. No action taken.\n",
      "DWH load completed successfully (incremental)!\n"
     ]
    }
   ],
   "source": [
    "load_data_into_warehouse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1179e8aa-85a5-4ceb-85a1-0f06d9708c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found last load date in control table: 2024-12-26 04:20:54.469924\n",
      "Last load date: 2024-12-26 04:20:54.469924\n",
      "Found periods to recalculate due to customers changes: [datetime.date(2018, 1, 1), datetime.date(2018, 2, 1), datetime.date(2018, 3, 1), datetime.date(2018, 4, 1), datetime.date(2018, 5, 1), datetime.date(2018, 6, 1), datetime.date(2018, 7, 1), datetime.date(2018, 8, 1), datetime.date(2018, 9, 1), datetime.date(2018, 10, 1), datetime.date(2018, 11, 1), datetime.date(2018, 12, 1), datetime.date(2019, 1, 1), datetime.date(2019, 2, 1), datetime.date(2019, 3, 1), datetime.date(2019, 4, 1), datetime.date(2019, 5, 1), datetime.date(2019, 6, 1), datetime.date(2019, 7, 1), datetime.date(2019, 8, 1), datetime.date(2019, 9, 1), datetime.date(2019, 10, 1), datetime.date(2019, 11, 1), datetime.date(2019, 12, 1), datetime.date(2020, 1, 1), datetime.date(2020, 2, 1), datetime.date(2020, 3, 1), datetime.date(2020, 4, 1), datetime.date(2020, 5, 1), datetime.date(2020, 6, 1), datetime.date(2020, 7, 1), datetime.date(2020, 8, 1), datetime.date(2020, 9, 1), datetime.date(2020, 10, 1), datetime.date(2020, 11, 1), datetime.date(2020, 12, 1), datetime.date(2021, 1, 1), datetime.date(2021, 2, 1), datetime.date(2021, 3, 1), datetime.date(2021, 4, 1), datetime.date(2021, 5, 1), datetime.date(2021, 6, 1), datetime.date(2021, 7, 1), datetime.date(2021, 8, 1), datetime.date(2021, 9, 1), datetime.date(2021, 10, 1), datetime.date(2021, 11, 1), datetime.date(2021, 12, 1), datetime.date(2022, 1, 1), datetime.date(2022, 2, 1), datetime.date(2022, 3, 1), datetime.date(2022, 4, 1), datetime.date(2022, 5, 1), datetime.date(2022, 6, 1), datetime.date(2022, 7, 1), datetime.date(2022, 8, 1), datetime.date(2022, 9, 1), datetime.date(2022, 10, 1), datetime.date(2022, 11, 1), datetime.date(2022, 12, 1)]\n",
      "Total unique periods to recalculate: [datetime.date(2018, 1, 1), datetime.date(2018, 2, 1), datetime.date(2018, 3, 1), datetime.date(2018, 4, 1), datetime.date(2018, 5, 1), datetime.date(2018, 6, 1), datetime.date(2018, 7, 1), datetime.date(2018, 8, 1), datetime.date(2018, 9, 1), datetime.date(2018, 10, 1), datetime.date(2018, 11, 1), datetime.date(2018, 12, 1), datetime.date(2019, 1, 1), datetime.date(2019, 2, 1), datetime.date(2019, 3, 1), datetime.date(2019, 4, 1), datetime.date(2019, 5, 1), datetime.date(2019, 6, 1), datetime.date(2019, 7, 1), datetime.date(2019, 8, 1), datetime.date(2019, 9, 1), datetime.date(2019, 10, 1), datetime.date(2019, 11, 1), datetime.date(2019, 12, 1), datetime.date(2020, 1, 1), datetime.date(2020, 2, 1), datetime.date(2020, 3, 1), datetime.date(2020, 4, 1), datetime.date(2020, 5, 1), datetime.date(2020, 6, 1), datetime.date(2020, 7, 1), datetime.date(2020, 8, 1), datetime.date(2020, 9, 1), datetime.date(2020, 10, 1), datetime.date(2020, 11, 1), datetime.date(2020, 12, 1), datetime.date(2021, 1, 1), datetime.date(2021, 2, 1), datetime.date(2021, 3, 1), datetime.date(2021, 4, 1), datetime.date(2021, 5, 1), datetime.date(2021, 6, 1), datetime.date(2021, 7, 1), datetime.date(2021, 8, 1), datetime.date(2021, 9, 1), datetime.date(2021, 10, 1), datetime.date(2021, 11, 1), datetime.date(2021, 12, 1), datetime.date(2022, 1, 1), datetime.date(2022, 2, 1), datetime.date(2022, 3, 1), datetime.date(2022, 4, 1), datetime.date(2022, 5, 1), datetime.date(2022, 6, 1), datetime.date(2022, 7, 1), datetime.date(2022, 8, 1), datetime.date(2022, 9, 1), datetime.date(2022, 10, 1), datetime.date(2022, 11, 1), datetime.date(2022, 12, 1)]\n",
      "Number of periods to recalculate: 60\n",
      "Read from DWH:\n",
      "    - 2997 craftsmen records\n",
      "    - 2997 customer records\n",
      "    - 2994 product records\n",
      "    - 2997 order records\n",
      "Calculating metrics...\n",
      "Updating datamart...\n",
      "Updating periods: ['2018-01', '2018-02', '2018-03', '2018-04', '2018-05', '2018-06', '2018-07', '2018-08', '2018-09', '2018-10', '2018-11', '2018-12', '2019-01', '2019-02', '2019-03', '2019-04', '2019-05', '2019-06', '2019-07', '2019-08', '2019-09', '2019-10', '2019-11', '2019-12', '2020-01', '2020-02', '2020-03', '2020-04', '2020-05', '2020-06', '2020-07', '2020-08', '2020-09', '2020-10', '2020-11', '2020-12', '2021-01', '2021-02', '2021-03', '2021-04', '2021-05', '2021-06', '2021-07', '2021-08', '2021-09', '2021-10', '2021-11', '2021-12', '2022-01', '2022-02', '2022-03', '2022-04', '2022-05', '2022-06', '2022-07', '2022-08', '2022-09', '2022-10', '2022-11', '2022-12']\n",
      "Records retained after filtering: 0\n",
      "Total records to write: 2948\n",
      "Written 2948 records to datamart\n",
      "Updated control table with load timestamp: 2024-12-26 04:22:55.440891\n",
      "Datamart update completed successfully!\n"
     ]
    }
   ],
   "source": [
    "load_data_into_datamart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58c7cab-8f83-4f45-9165-203bcb6a4e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "Вывод: Где-то есть баг, который не позволяет обновить витрину одной записью. При этом в DWH пишется корректно"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
